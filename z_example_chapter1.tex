\renewcommand{\publ}{}

% \index{Classification!LVQ}

\chapter{Introduction}

\PARstart{D}ue to advanced sensor technology, rapidly increasing digitalization capabilities and the availability of less and 
less expensive storage volume the amount of data has grown tremendously in the last decades. 
In the years between 1999 and 2002 an increase of stored information about 30\% each year was estimated \cite{Lyman2003}. 
Usually this data consists of a variety of measured features leading to also very high dimensional data sets. 
Manually inspection of the data becomes more costly and automatic methods to help humans to quickly scan through 
massive data amounts are desirable. 
This gave rise to many applications in computer science to process the available data: 
advanced techniques including data mining \cite{Han2005}, pattern recognition \cite{Duda2000} and machine learning 
\cite{Mitchell1997,Ripley1996,Bishop2007}, among others. 
Even with great progress in those fields the optimization of existing methods and development of novel schemes is highly 
desirable to perform faster and more efficient data analysis. 

The field of machine learning concerns the design of algorithms, 
which aim at the optimization of adaptive systems on the basis of example data. 
A model is adapted to learn complex patterns and process new data coming from the same domain better regarding the specified objective. 
The analysis of patterns involves a number of tasks including data representation, classification, clustering, density estimation, 
regression, feature extraction and dimension reduction, just to name a few. 
A lot of data visualization tools have been developed to use cognitive capabilities of humans for structure 
detection in visual images. 
Structural characteristics of the data can be captured almost instantly by humans despite the amount of data points which 
are represented in the visualization. 
Hence, dimension reduction and visualization are commonly used modern data mining techniques \cite{Lee2007}. 
Machine learning is broadly categorized into reinforcement, supervised and unsupervised learning. 
Reinforcement learning is inspired by behaviorist psychology and concerns the finding of suitable actions to maximize some notion 
of reward \cite{Sutton1998}. 
Supervised techniques involve external supervision, which provides correct responses to the given inputs. 
The aim is usually the discrimination of the categories and to maximize the generalization for novel data. 
Unsupervised methods, on the other hand, do not need supervision and their goal is the discovery of underlying structures 
and regularities based on the definition of some basic properties of the data.
An elaborate description concerning the history of machine learning can be found in, e.\ g. 
\cite{Bishop1995,Ripley1996,Mitchell1997,Duda2000,Bishop2007}. 

A very intuitive supervised technique called \ac{$k$-NN} classifier compares the unknown data to all known examples with 
respect to some dissimilarity measure \cite{Duda2000}. 
Obviously the computational effort and memory usage scales with the number of known samples. 
Therefore prototype-based techniques were developed, which employ representations of data subsets. 
The prototypes are vector locations in the feature space. 
They usually serve as typical representatives and reflect the characteristics of the data in their direct neighborhood. 
% Supervised and unsupervised methods can be used to represent the data by means of prototypes. 
Some prominent unsupervised examples are the \ac{SOM} \cite{Kohonen2001} and \ac{NG} \cite{Martinetz1991}. 
And a popular supervised family of such prototype-based classification methods is \ac{LVQ} \cite{Kohonen2001}. 
% The set of prototypes and the dissimilarity measure parameterize the model. 
All these methods crucially depend on the distance measure, which is used to adapt the prototype positions and 
performs the nearest prototype classification. 
Therefore the learning of adaptive metrics with respect to the given problem at hand was investigated \cite{Xing2002,Chopra2005,Frome2007,Schneider2009a,Schneider2009b}. 

This thesis investigates adaptive dissimilarities and applications varying from classification up to supervised and unsupervised 
dimension reduction. 

\section{Scope of this thesis}

The objective of this thesis is manifold, it contains:
\begin{itemize}
\item the introduction of prototype-based adaptive dissimilarity learning with limited rank matrices, 
\item a new method based on that principle for learning in complex valued data domains and 
\item a general view and new algorithms for unsupervised as well as supervised dimension reduction and visualization. 
\end{itemize}
%for prototype-based adaptive dissimilarity learning
Adaptive dissimilarities are a powerful tool, which are shown to improve the performance of supervised methods, such as 
for example \ac{LVQ} and the \ac{$k$-NN} classifiers. 
These classification algorithms crucially depend on the distance measure used. 
Metric adaptation techniques allow the learning of discriminative dissimilarity measures from a given set of representative example data. 
Restrictions in adaptive matrix learning, e.\ g. the limitation of the rank, enables  
the learning of discriminative global or local linear transformations. 
These transformations can then be used for supervised dimension reduction and visualization. 
It also reduces the number of the effective learning parameters, 
which might be interesting from the computational point of view. 

In the first part of this contribution previously proposed methods for metric learning in \ac{LVQ} are extended to limited rank matrices. 
Several practical applications are investigated including \ac{CBIR}, dimension reduction and visualization. 
Furthermore we provide an extension which can be used on complex valued data shown on an example for texture classification in images. 

The second part of this thesis focuses on dimension reduction and visualization. 
We provide a general view on existing dimension reduction methods, 
which originally provide just an implicit mapping of the given data points itself. 
Based on this general principle we extend these methods to learn the parameters of explicit mapping functions instead. 
This provides direct out-of-sample extensions, reduces computational effort by restricting the learning process just on a small 
subset of the possible large data set and enables the formal investigation of the generalization ability. 
Furthermore we provide an unsupervised dimension reduction method, which in contrast to other techniques exhibit 
a complexity which scales linear with the number of data points in every step. 
It aims in the combination of fast online learning with the high quality of direct divergence optimization, 
successfully used by state-of-the-art techniques.  

\section{Outline}

% This section briefly addresses the outline of the thesis and the topics of the chapters. 
% The thesis is divided into two parts.  
% Part \ref{part:1} spans from Chapters 2 to 4 and  discusses adaptive dissimilarity measures especially as extensions of \ac{LVQ}. 
% The metric learning defined in this work can be reformulated to learn global or local linear projections of 
% the data, which smoothly leads over to Part \ref{part:2} of the thesis dealing with dimension reduction. 
% 
% The chapters are organized as follows: 
% Chapter \ref{chapter:LVQ} provides a short introduction to prototype-based learning and adaptive dissimilarities. 
% Basic algorithms like \ac{GLVQ} and \ac{GMLVQ} are described in detail. 
% The metric adaptation scheme is then modified to use limited rank matrices, which reduce the number of parameters and 
% thus the computational effort and gives direct access to supervised dimension reduction. 
% The latter aspect is resumed and investigated in more detail in Chapter \ref{chapter:discriminative_DR} 
% in the second part of this thesis. 
% 
% In Chapter \ref{chapter:CBIR} adaptive dissimilarity learning is used in an application for \ac{CBIR} in Dermatology.  
% The aim is a computer aided diagnosis system which helps the user, e.\ g. medical doctors, with targeted searches in image data bases. 
% A learned discriminative distance measure is used to retrieve an arbitrary number of most similar pictures from a data base of images 
% of skin lesions. 
% Two methods for metric learning are used and compared: \ac{LMNN}, which bases on the \ac{$k$-NN} algorithm, and the \ac{LVQ} based
% approach. 
% It is shown, that adaptive dissimilarities can be used to improve the performance of a \ac{CBIR} system. 
% 
% Chapter \ref{chapter:CIA_LVQ} introduces a variant of \ac{LVQ} defined on complex valued data. 
% The modification is shown on one example application for texture classification in color images. 
% These variant called \ac{CIA LVQ} combines well known image analysis filter techniques with prototype-based transformation learning 
% defined in the Fourier domain. 
% 
% % The introduction to the second part of the thesis concerning 
% Chapter \ref{chapter:DiReduct} provides an introduction to the second part of the thesis: dimension reduction and visualization. 
% An overview over existing techniques is given and a general principle is formulated. 
% Based on that principle a general framework is proposed which extends given dimension reduction techniques to learn an 
% explicit mapping function. 
% This way those methods, which are originally introduced to provide implicit point-to-point embeddings can 
% be extended to learn mapping functions instead. 
% Out-of-Sample extensions become immediate, the investigation of the generalization ability is possible and it can save computational 
% effort, because the mapping function can be learned on a representative small subset of the data. 
% 
% In Chapter \ref{chapter:discriminative_DR} the adaptive distances and discriminative transformations introduced in Chapter 
% \ref{chapter:LVQ} are used for supervised dimension reduction and visualization. 
% A variety of given unsupervised techniques are extended to use label information by plugging in the supervised learned 
% distance or the local linear transformations. 
% 
% Most dimension reduction techniques preserve properties extracted from local neighborhoods. 
% This requires the computation of pairwise distances, so the computational effort squares with the number of points. 
% Chapter \ref{chapter:SONE} introduces a dimension reduction method which combines the high performance of 
% direct divergence optimization with fast online learning, leading to a complexity growing linear with the number of points. 
% There are numerous divergences offering different properties. Chapter \ref{chapter:divergences} gives an overview over the three
% divergence
% families and examples thereof. Using the concept of Fr\'{e}chet derivatives three algorithms are expanded to the use of arbitrary
% divergences. 
% 
% Finally, Chapter \ref{chapter:conclusion} presents a brief summary of the research and a collection of ideas for future work and 
% investigation. 
